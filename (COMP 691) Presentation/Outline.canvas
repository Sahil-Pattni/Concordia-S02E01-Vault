{
	"nodes":[
		{"id":"f8b208b43d3280d0","type":"text","text":"# PRESENTATION","x":-287,"y":-30,"width":312,"height":50,"color":"5"},
		{"id":"147654377456fbe2","type":"text","text":"## 1. Domain / Relevant Background","x":-811,"y":-495,"width":524,"height":60},
		{"id":"03b50fa32d61097f","type":"text","text":"## 3. Main Contributions\n> [!Abstract] Novel Architecture for Video Compression\n> 1. Generalizes motion estimation to perform learned compensation beyond simple translations.\n> 2. Maintains a state of arbitrary information learned by the model (instead of strictly relying on previously transmitted frames).\n> 3. Enables jointly compressing all transmitted signals (such as optical flow and residual).\n\n>[!info]\n> The setting for this is low-latency modes where each frame can only rely on past information.","x":360,"y":-199,"width":524,"height":519},
		{"id":"a90ae20f02bcba8f","type":"text","text":"## 2. Problem Statement & Challenges\n\n>[!question] What's wrong with traditional video codecs?\n>1. Purely motion-based translation.\n>2. Knowledge limited to optical flow maps and reference frames.\n>3. Bandwidth distribution between motion and residual data.\n>4. Block matching:\n>\t1. All pixels in a group are assumed to have same motion vector.\n>\t2. Details of how pixels move are also limited to fixed positions within pixel (discrete step size).\n","x":-120,"y":-740,"width":680,"height":380},
		{"id":"28bc3aae34006146","type":"text","text":"## 6. Limitations (3 points required)","x":-1362,"y":-217,"width":524,"height":36,"color":"1"},
		{"id":"6f3ce796a587a27f","type":"text","text":"## 7. Potential Improvements (3 points req.)","x":-1362,"y":-16,"width":524,"height":60,"color":"1"},
		{"id":"b57738249edb33c9","type":"text","text":"### Compensation Beyond Translation\nTraditional video compression methods mainly focus on how things move from one frame to the next (i.e. motion).\n> [!example] A person walking from the left to the right of the frame\n> Traditional techniques work well here because the motion is predictable.\n\nThere are - however - cases of complex motion where such techniques fail to perform effectively.\n> [!example] A person turning their head sideways\n> Traditional techniques fail here because their prediction is limited to movement only. They can't predict what the side of a person's face looks when they've only seen the front.\n\nThe proposed system mitigates these shortcomings, as it can learn and understand not just movement, but also more complex patterns of changes in video.","x":1120,"y":-1340,"width":640,"height":581},
		{"id":"579b3308f95fedbc","type":"text","text":"### Propagation of a Learned State\nIn traditional codecs, the knowledge about the change is transmitted through optical flow maps and reference frames. These representations are simple and cannot represent a wide range of information. Furthermore, they don't have a good memory for long-term changes.\n\nIn contrast, the proposed system uses a more advanced state of memory that can hold a wider range of information and lover a longer time. This in turn leads to more effective video compression.","x":1120,"y":-672,"width":640,"height":354},
		{"id":"937badc8194c73c2","type":"text","text":"### Frame Types\n- I-Frame\n- P-Frame\n- B-Frame","x":-548,"y":-795,"width":403,"height":163},
		{"id":"1378b15088e3f80e","type":"text","text":"### P-Frames\n","x":-466,"y":-900,"width":240,"height":54},
		{"id":"2a9652e0fb59ba11","type":"text","text":"#### Step 2: Residual Compression\nConsider the motion compensated approximation of the frame from step 1 as $m_t$. We compress *the difference* between the target frame and $m_t$.\n\n> [!example] Formally\n> $\\Delta_t = x_t - m_t$\n\nThe residual ($\\Delta_t$) is *independently encoded* with an image compression algorithm adapted to the sparsity of the residual.","x":-287,"y":-1400,"width":540,"height":380},
		{"id":"d532b78b18d58115","type":"text","text":"#### Step 1: Motion Compensation\nConsider the current video frame as $x_t$. Instead of transmitting the entire $x_t$ frame, we can compare parts of $x_t$ to previous reference frames.\n\nWe can figure out how much of the current frame as \"*moved*\" compared to the previous frames. We denote this movement information in an **optical flow map** denoted as $f_t$.\n\nTo save on data/processing, not every minute detail of what changed has to be transmitted. Similar areas can be grouped together into **blocks** and transmit information about the larger blocks that have moved.","x":-1006,"y":-1400,"width":540,"height":380},
		{"id":"1a704afde5ea188a","type":"text","text":"### Flexible Motion Field Representation\n> [!question] How do traditional codecs represent motion?\n\nTraditional codecs represents the motion of a **block** of pixels, where the motion vector for all pixels in the same block are assumed to be the same. Additionally, the step size for the motion vector is discrete (*i.e.* the movement details are limited to specific, fixed positions within a pixel). This is efficient, but this system of representation cannot capture fine and complex motion.\n\nIn contrast, the proposed system can distribute bandwidth according to frame complexity so that more complex motions can be captured, while also being very efficient when representing simple or unimportant motion.","x":1120,"y":-236,"width":860,"height":360},
		{"id":"360f54a5e48a1ddf","type":"text","text":"### Multi-Flow Representation\nConsider a video of a train moving between fine branches of a tree. Traditional codecs cannot represent this motion in a simple flow map, as there are *small occlusion patterns that break the flow*. Additionally, this occluded content will have to be synthesized again once it reappears.\n\nThe proposed system allows the flexibility to decompose a complex scene into a *mixture* of multiple simple flows and preserves the occluded content.\n\n>[!info] Occluded content\n> The pixels of the train that seemingly *disappear* when a branch obstructs it is considered to be *occluded*.\n> Once it passes the branch, these pixels seemingly re-appear.\n","x":1120,"y":204,"width":860,"height":400},
		{"id":"0aadc96abe21781c","type":"text","text":"### Spatial Rate Control\nTo efficiently compress a video, it is critical to use more data (bitrate allocated) for more detailed parts of a frame, and less data for other parts. In ML-based video compression techniques, it has been challenging to create a *single* model that can adjust and allocate different amounts of data for multiple bitrates *and* achieve the same results as separate, distinct models for each bitrate.\n\nThe proposed system manages to do just this.","x":1120,"y":644,"width":860,"height":240},
		{"id":"08408e92dd42de77","type":"text","text":"### Joint Compression of Motion and Residual\nEach codec must decide **how to distribute the bandwidth** between the information about the motion (*how things are moving*) and information about the residual (*what the actual picture looks like*). \n>[!warning] However, the best balance between these two varies from one frame to another.\n\nTraditional codecs compress the motion and actual image separately, without an easy way to adjust the distribution ratio. In contrast, the proposed system combines the compression of both these signals via the same *bottleneck*. This way **the system can learn how to adapt the bitrate distribution based on how complex a given frame is**.\n\n>[!example] Formally\n> $f_c$ = The complexity of a frame.\n> $m$ = Motion compensation information.\n> $r$ = Residual information.\n> $b(x)$ = The function that distributes the bitrate. \n>  \n>  ---\n>  \n>  $b(f_c) = m^x \\cdot r^{1-x}$\n","x":1120,"y":933,"width":860,"height":620},
		{"id":"f2c5797ff36184fd","type":"text","text":"## Possible Questions\n\n>[!question]  Potential question that could be asked.\n> An explanation by me.\n>","x":-2120,"y":204,"width":524,"height":576,"color":"1"},
		{"id":"f805373f663d7e59","type":"text","text":"## 5. Experimental Results\n> [!Abstract] \n> On standard- definition (SD) videos, HEVC/H.265, AVC/H.264 and VP9 typically produce codes up to 60% larger than the paper's algorithm.\n> \n> On high-definition (HD) 1080p videos, H.265 and VP9 typically produce codes up to 20% larger, and H.264 up to 35% larger.","x":-1362,"y":192,"width":524,"height":360,"color":"1"},
		{"id":"bc93b02cbbdebe94","type":"text","text":"## 4. The Solution (In Detail)","x":-393,"y":244,"width":524,"height":60,"color":"1"},
		{"id":"d430274d467240eb","type":"text","text":"### Transition from traditional to ML\n#### Step 1\n![[Step 1.png|550]]\n\n- $M(\\cdot)$ (first yellow node) is learnable flow estimator network. This outputs an inverse optical flow $f_t \\in \\mathbb{R}^{2 \\times H \\times W}$.\n- $F(\\cdot,\\cdot)$ is the optical flow operator.\n- Blue tensors are frames and green tensors are flows.\n- Grey operators are hard-coded differentiable functions (activation functions?).\n- $E_f$ is an encoder, $D_f$ is a decoder. This is used to auto-encode $f_t$ through a low-bandwidth bottleneck, the reconstructed flow is $\\hat{f_t}$.\n- Traditionally this can be further optimized by only encoding the difference ($f_t -\\hat{f_t}$) from the previously reconstructed flow.\n- Use this to compute the motion-compensated flow $m_t = F(\\hat{x}_{t-1},\\hat{f_t})$.\n- Build a *residual compressor* with learnable encoder $E_r$ and decoder $D_r$. This calculates the residual $\\Delta_t = x_t - m_t$.\n- Agnostic to compression architectures, any SotA ML image architecture can be used.","x":166,"y":933,"width":718,"height":820},
		{"id":"b98478d3a8a38139","type":"text","text":"### Step 2: Joint Compression of Flow and Residual\n![[Step 2.png|550]]\n\nIn Step 1, we encoded the flow and residual separately. In step 2, we compress them through a single bottleneck. In this step, we jointly encode them through the same bottleneck.\n\n","x":166,"y":1813,"width":718,"height":600},
		{"id":"23b0e6a739024a86","x":166,"y":2513,"width":718,"height":847,"type":"text","text":"### Step 3: Learnable State $S_t$\n![[Step 3.png|600]]\nNotice that all the prior memory being propagated from frame to frame is done so through:\n1. The previously constructed frame $\\hat{x}_{t-1}$\n2. The flow $\\hat{f}_{t-1}$\nBoth of these are *embedded in raw pixel space*. These representations are computationally inefficient and inexpressive.\n\n\nThe state $S_t$ can be thought of as an extension of an RNN, where $S_t$ accumulates temporal information. Unlike an RNN, however, updates to $S_t$ must pass through a low-bandwidth bottleneck.\n\n$G(\\cdot)$ is a module that computes the frame reconstruction $\\hat{x}_t$ from the updated state $S_t$.\n\n---\n**Step 4**: Arbitrary compensation\n\n$G(\\cdot)$ still simulates traditional flow-based approaches, and therefore only allows for compensations of simple transformations (*i.e. pixels can be moved around, but their values cannot be arbitrarily changed*).\nWe can generalize $G(\\cdot)$ to generate multiple flows instead of a single one, and multiple reference frames to which the flows can be applied."},
		{"id":"88d21e259f974797","x":-460,"y":932,"width":560,"height":268,"type":"text","text":"###  Building Blocks\nMulti-scale dual path backbone is the fundamental building block for the architecture. It is the combination of the multi-scale rendition of DenseNet with dual path.\n>[!warning] Look up these references.\n\n"},
		{"id":"3d8efb9abe44e82e","type":"text","text":"### Model Architecture\n![[pseudocode.png|600]]\n","x":-40,"y":434,"width":718,"height":420},
		{"id":"511be8c9a90c368c","x":-1260,"y":680,"width":740,"height":280,"type":"text","text":"### Coding Procedure\n>[!abstract]\n>After applying the encoder network, we have a fixed-size tensor $c \\in [-1,1]^{C \\times Y \\times X}$.\nWe want to map $c$ to a bitstream $e \\in {0,1}^{\\ell(e)}$, where $\\ell(e)$ is the variable length. The coder is expected to achieve a high coding efficiency by exploiting the redundancy injected into $c$ by the regularizer during training."},
		{"id":"c15bd7236e0f41fb","x":-1260,"y":1053,"width":740,"height":307,"type":"text","text":"#### Step 1: Bitplane Decomposition\nTransform $c$ into a binary tensor $b \\in {0,1}^{B \\times C \\times Y \\times X}$ by decomposing it into $B$ bitplanes. It does this by mapping each value $c_{chw}$ into its binary expansion $b_{1chw},\\cdots,b_{Bchw}$ of $B$ bits.\n>[!note]\n>This is a lossy operation, since the precision of each value is truncated.\n>In practice, they use $B = 6$.\n\n"},
		{"id":"0344ebf5e2c9162c","x":-1260,"y":1400,"width":740,"height":260,"type":"text","text":"#### Step 2: Adaptive Entropy Coding (AEC)\nThe AEC maps the binary tensor $b$ into a bitstream $e$.\n\nA classifier is trained to compute the probability of activation $\\mathbb{P}[b_{bcyx} = 1 \\vert C \\ ]$ for each bit $b_{bcyx}$ conditioned by some context $C$.\n\nBy considering the bits the classifier has seen before ($C$), the compression is more efficient because it leverages the structure and patterns in the data to represent it with fewer bits."},
		{"id":"d14cc43d3bc2fe88","x":-1260,"y":1740,"width":740,"height":560,"type":"text","text":"#### Step 3: Adaptive Code-length Regularization\nThe regularizer is designed to reduce the entropy content of $b$ in a way that can be leveraged by the entropy coder. \n\n$\\mathscr{R}(\\hat{c}) = \\frac{\\alpha_i}{CYX} \\displaystyle\\sum_{cyx}^{} log \\vert \\hat{c}_{cyx} \\vert$\nfor iteration $i$ and scalar $\\alpha_i$.\nThis shapes the distribution of bits in a code-layer ($\\hat{c}$) to be increasingly sparse as it iterates through the bitplanes.\n\nThe scalar $\\alpha_i$ is adjusted at each training iteration in an attempt to make the outputted encoded length match the target length.\n\n$\\mathbb{E}_{\\hat{c}} = [ \\ell(e) ] \\rightarrow \\ell^{\\text{target}}$\n\nDuring training, the system monitors the average codelength. Codelength refers to the length of the compressed data. The goal is to make this length match a specific target value $\\ell^{\\text{target}}$. If the actual code-length is longer or shorter than the target, the scalar $\\alpha_i$ is adjusted using a feedback loop to make it more or less aggressive in reducing entropy, so that the code-length gets closer to the target.\n\n"},
		{"id":"ab03a60a1b6e9696","x":-1940,"y":-1600,"width":794,"height":580,"type":"file","file":"(COMP 691) Presentation/Optical Flow Estimation.md"}
	],
	"edges":[
		{"id":"30625630b6fac357","fromNode":"f8b208b43d3280d0","fromSide":"right","toNode":"03b50fa32d61097f","toSide":"left"},
		{"id":"d4600e2f579fc882","fromNode":"147654377456fbe2","fromSide":"top","toNode":"937badc8194c73c2","toSide":"bottom"},
		{"id":"77be912b9d57f8bd","fromNode":"f8b208b43d3280d0","fromSide":"top","toNode":"147654377456fbe2","toSide":"bottom"},
		{"id":"edcb3949aaf34023","fromNode":"f8b208b43d3280d0","fromSide":"top","toNode":"a90ae20f02bcba8f","toSide":"bottom"},
		{"id":"7ba4d30394bf5ebc","fromNode":"f8b208b43d3280d0","fromSide":"left","toNode":"28bc3aae34006146","toSide":"right"},
		{"id":"e217b3b6ac8a6030","fromNode":"f8b208b43d3280d0","fromSide":"left","toNode":"6f3ce796a587a27f","toSide":"right"},
		{"id":"1d21176369cbfa60","fromNode":"f8b208b43d3280d0","fromSide":"bottom","toNode":"bc93b02cbbdebe94","toSide":"top"},
		{"id":"e9e70a6df665da9f","fromNode":"f8b208b43d3280d0","fromSide":"bottom","toNode":"f2c5797ff36184fd","toSide":"top"},
		{"id":"1dbf19d0f3fd630d","fromNode":"937badc8194c73c2","fromSide":"top","toNode":"1378b15088e3f80e","toSide":"bottom"},
		{"id":"7e68669315e2b9c2","fromNode":"1378b15088e3f80e","fromSide":"top","toNode":"d532b78b18d58115","toSide":"bottom"},
		{"id":"2d1d7be85c77bd6a","fromNode":"1378b15088e3f80e","fromSide":"top","toNode":"2a9652e0fb59ba11","toSide":"bottom"},
		{"id":"824a93311ce8b771","fromNode":"d532b78b18d58115","fromSide":"right","toNode":"2a9652e0fb59ba11","toSide":"left","color":"6"},
		{"id":"41dfd8cecdbdb75c","fromNode":"03b50fa32d61097f","fromSide":"right","toNode":"b57738249edb33c9","toSide":"left"},
		{"id":"7fc88214c1719fcd","fromNode":"03b50fa32d61097f","fromSide":"right","toNode":"579b3308f95fedbc","toSide":"left"},
		{"id":"f0ba91b4a9574f7f","fromNode":"03b50fa32d61097f","fromSide":"right","toNode":"08408e92dd42de77","toSide":"left"},
		{"id":"9483e4963fdf2832","fromNode":"f8b208b43d3280d0","fromSide":"bottom","toNode":"f805373f663d7e59","toSide":"top"},
		{"id":"cebc946c2bd6720c","fromNode":"b57738249edb33c9","fromSide":"left","toNode":"2a9652e0fb59ba11","toSide":"right","fromEnd":"arrow","color":"6","label":"improvement"},
		{"id":"55b322e6cc050ec0","fromNode":"03b50fa32d61097f","fromSide":"right","toNode":"1a704afde5ea188a","toSide":"left"},
		{"id":"783dff047806e902","fromNode":"1a704afde5ea188a","fromSide":"right","toNode":"08408e92dd42de77","toSide":"right","color":"6"},
		{"id":"5f915f6697326acf","fromNode":"03b50fa32d61097f","fromSide":"right","toNode":"360f54a5e48a1ddf","toSide":"left"},
		{"id":"9dfc5e1353194041","fromNode":"03b50fa32d61097f","fromSide":"right","toNode":"0aadc96abe21781c","toSide":"left"},
		{"id":"3b1ac509c98c8dcd","fromNode":"bc93b02cbbdebe94","fromSide":"bottom","toNode":"3d8efb9abe44e82e","toSide":"top"},
		{"id":"d083f1135fce1be1","fromNode":"3d8efb9abe44e82e","fromSide":"bottom","toNode":"d430274d467240eb","toSide":"top","color":"6"},
		{"id":"a602ecb3c8fec509","fromNode":"147654377456fbe2","fromSide":"top","toNode":"ab03a60a1b6e9696","toSide":"bottom"},
		{"id":"ffd3819d35cfb2a5","fromNode":"d430274d467240eb","fromSide":"bottom","toNode":"b98478d3a8a38139","toSide":"top"},
		{"id":"f586ff3a8a05576c","fromNode":"b98478d3a8a38139","fromSide":"right","toNode":"08408e92dd42de77","toSide":"left","toEnd":"none","color":"6"},
		{"id":"399c63b71f945699","fromNode":"b98478d3a8a38139","fromSide":"bottom","toNode":"23b0e6a739024a86","toSide":"top","color":"6"},
		{"id":"1bf62219c19355d4","fromNode":"3d8efb9abe44e82e","fromSide":"bottom","toNode":"88d21e259f974797","toSide":"top"},
		{"id":"d7dda108447f665b","fromNode":"bc93b02cbbdebe94","fromSide":"bottom","toNode":"511be8c9a90c368c","toSide":"top"},
		{"id":"e8a31e712bce46e6","fromNode":"511be8c9a90c368c","fromSide":"bottom","toNode":"c15bd7236e0f41fb","toSide":"top"},
		{"id":"976fe5838658e297","fromNode":"c15bd7236e0f41fb","fromSide":"bottom","toNode":"0344ebf5e2c9162c","toSide":"top"},
		{"id":"948e2c317667d1b8","fromNode":"0344ebf5e2c9162c","fromSide":"bottom","toNode":"d14cc43d3bc2fe88","toSide":"top"}
	]
}